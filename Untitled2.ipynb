{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO7gN2ZIluHoi7m31B8CHkt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MadeehaTassadaq/AI-VOICE-CHATBOT/blob/main/Untitled2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "di-tTTc9QxL_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c97294fd-7394-468f-aca6-b390a3d94f3a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.11/dist-packages (0.8.4)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (0.6.15)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.19.2)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.160.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.27.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (5.29.3)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.10.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.12.2)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (1.67.0)\n",
            "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (2.32.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9)\n",
            "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (4.1.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (2.27.2)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.70.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.62.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.1)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (2025.1.31)\n",
            "Requirement already satisfied: llama-index in /usr/local/lib/python3.11/dist-packages (0.12.19)\n",
            "Requirement already satisfied: llama-index-agent-openai<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.4.6)\n",
            "Requirement already satisfied: llama-index-cli<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.4.0)\n",
            "Requirement already satisfied: llama-index-core<0.13.0,>=0.12.19 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.12.19)\n",
            "Requirement already satisfied: llama-index-embeddings-openai<0.4.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.3.1)\n",
            "Requirement already satisfied: llama-index-indices-managed-llama-cloud>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.6.7)\n",
            "Requirement already satisfied: llama-index-llms-openai<0.4.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.3.20)\n",
            "Requirement already satisfied: llama-index-multi-modal-llms-openai<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.4.3)\n",
            "Requirement already satisfied: llama-index-program-openai<0.4.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.3.1)\n",
            "Requirement already satisfied: llama-index-question-gen-openai<0.4.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.3.0)\n",
            "Requirement already satisfied: llama-index-readers-file<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.4.5)\n",
            "Requirement already satisfied: llama-index-readers-llama-parse>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.4.0)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.11/dist-packages (from llama-index) (3.9.1)\n",
            "Requirement already satisfied: openai>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (1.61.1)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.19->llama-index) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy>=1.4.49 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.19->llama-index) (2.0.38)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.19->llama-index) (3.11.12)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.19->llama-index) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.19->llama-index) (1.2.18)\n",
            "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.19->llama-index) (1.0.8)\n",
            "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.19->llama-index) (1.2.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.19->llama-index) (2024.10.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.19->llama-index) (0.28.1)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.19->llama-index) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.19->llama-index) (3.4.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.19->llama-index) (1.26.4)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.19->llama-index) (10.4.0)\n",
            "Requirement already satisfied: pydantic>=2.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.19->llama-index) (2.10.6)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.19->llama-index) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.19->llama-index) (9.0.0)\n",
            "Requirement already satisfied: tiktoken>=0.3.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.19->llama-index) (0.9.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.19->llama-index) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.19->llama-index) (4.12.2)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.19->llama-index) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.19->llama-index) (1.17.2)\n",
            "Requirement already satisfied: llama-cloud<0.2.0,>=0.1.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-indices-managed-llama-cloud>=0.4.0->llama-index) (0.1.12)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (4.13.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2.2.2)\n",
            "Requirement already satisfied: pypdf<6.0.0,>=5.1.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (5.3.0)\n",
            "Requirement already satisfied: striprtf<0.0.27,>=0.0.26 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (0.0.26)\n",
            "Requirement already satisfied: llama-parse>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-llama-parse>=0.4.0->llama-index) (0.6.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index) (2024.11.6)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.19->llama-index) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.19->llama-index) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.19->llama-index) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.19->llama-index) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.19->llama-index) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.19->llama-index) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.19->llama-index) (1.18.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2.6)\n",
            "Requirement already satisfied: certifi>=2024.7.4 in /usr/local/lib/python3.11/dist-packages (from llama-cloud<0.2.0,>=0.1.8->llama-index-indices-managed-llama-cloud>=0.4.0->llama-index) (2025.1.31)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.19->llama-index) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.19->llama-index) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.19->llama-index) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.13.0,>=0.12.19->llama-index) (0.14.0)\n",
            "Requirement already satisfied: llama-cloud-services>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index) (0.6.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (0.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (1.3.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.19->llama-index) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.19->llama-index) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.19->llama-index) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.19->llama-index) (2.3.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.19->llama-index) (3.1.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.13.0,>=0.12.19->llama-index) (1.0.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json->llama-index-core<0.13.0,>=0.12.19->llama-index) (3.26.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2025.1)\n",
            "Requirement already satisfied: python-dotenv<2.0.0,>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from llama-cloud-services>=0.6.1->llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index) (1.0.1)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.11/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.13.0,>=0.12.19->llama-index) (24.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (1.17.0)\n",
            "Requirement already satisfied: llama-index-embeddings-gemini in /usr/local/lib/python3.11/dist-packages (0.3.1)\n",
            "Requirement already satisfied: google-generativeai>=0.5.2 in /usr/local/lib/python3.11/dist-packages (from llama-index-embeddings-gemini) (0.8.4)\n",
            "Requirement already satisfied: llama-index-core<0.13.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-embeddings-gemini) (0.12.19)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.11/dist-packages (from google-generativeai>=0.5.2->llama-index-embeddings-gemini) (0.6.15)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.11/dist-packages (from google-generativeai>=0.5.2->llama-index-embeddings-gemini) (2.19.2)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (from google-generativeai>=0.5.2->llama-index-embeddings-gemini) (2.160.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-generativeai>=0.5.2->llama-index-embeddings-gemini) (2.27.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from google-generativeai>=0.5.2->llama-index-embeddings-gemini) (5.29.3)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (from google-generativeai>=0.5.2->llama-index-embeddings-gemini) (2.10.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from google-generativeai>=0.5.2->llama-index-embeddings-gemini) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from google-generativeai>=0.5.2->llama-index-embeddings-gemini) (4.12.2)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai>=0.5.2->llama-index-embeddings-gemini) (1.26.0)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-gemini) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy>=1.4.49 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-gemini) (2.0.38)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-gemini) (3.11.12)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-gemini) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-gemini) (1.2.18)\n",
            "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-gemini) (1.0.8)\n",
            "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-gemini) (1.2.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-gemini) (2024.10.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-gemini) (0.28.1)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-gemini) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-gemini) (3.4.2)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-gemini) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-gemini) (1.26.4)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-gemini) (10.4.0)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-gemini) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-gemini) (9.0.0)\n",
            "Requirement already satisfied: tiktoken>=0.3.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-gemini) (0.9.0)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-gemini) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-gemini) (1.17.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-gemini) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-gemini) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-gemini) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-gemini) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-gemini) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-gemini) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-gemini) (1.18.3)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai>=0.5.2->llama-index-embeddings-gemini) (1.67.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai>=0.5.2->llama-index-embeddings-gemini) (5.5.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai>=0.5.2->llama-index-embeddings-gemini) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai>=0.5.2->llama-index-embeddings-gemini) (4.9)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-gemini) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-gemini) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-gemini) (2024.11.6)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai>=0.5.2->llama-index-embeddings-gemini) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai>=0.5.2->llama-index-embeddings-gemini) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-gemini) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-gemini) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-gemini) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-gemini) (2025.1.31)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-gemini) (3.1.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-gemini) (1.0.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-gemini) (3.26.1)\n",
            "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai>=0.5.2->llama-index-embeddings-gemini) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai>=0.5.2->llama-index-embeddings-gemini) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai>=0.5.2->llama-index-embeddings-gemini) (4.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-gemini) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-gemini) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-gemini) (0.14.0)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai>=0.5.2->llama-index-embeddings-gemini) (1.70.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai>=0.5.2->llama-index-embeddings-gemini) (1.62.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai>=0.5.2->llama-index-embeddings-gemini) (3.2.1)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.11/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-gemini) (24.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai>=0.5.2->llama-index-embeddings-gemini) (0.6.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-gemini) (1.3.1)\n",
            "Requirement already satisfied: llama-index-llms-gemini in /usr/local/lib/python3.11/dist-packages (0.4.10)\n",
            "Requirement already satisfied: google-generativeai>=0.5.2 in /usr/local/lib/python3.11/dist-packages (from llama-index-llms-gemini) (0.8.4)\n",
            "Requirement already satisfied: llama-index-core<0.13.0,>=0.12.12 in /usr/local/lib/python3.11/dist-packages (from llama-index-llms-gemini) (0.12.19)\n",
            "Requirement already satisfied: pillow<11.0.0,>=10.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-llms-gemini) (10.4.0)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.11/dist-packages (from google-generativeai>=0.5.2->llama-index-llms-gemini) (0.6.15)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.11/dist-packages (from google-generativeai>=0.5.2->llama-index-llms-gemini) (2.19.2)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (from google-generativeai>=0.5.2->llama-index-llms-gemini) (2.160.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-generativeai>=0.5.2->llama-index-llms-gemini) (2.27.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from google-generativeai>=0.5.2->llama-index-llms-gemini) (5.29.3)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (from google-generativeai>=0.5.2->llama-index-llms-gemini) (2.10.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from google-generativeai>=0.5.2->llama-index-llms-gemini) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from google-generativeai>=0.5.2->llama-index-llms-gemini) (4.12.2)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai>=0.5.2->llama-index-llms-gemini) (1.26.0)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.12->llama-index-llms-gemini) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy>=1.4.49 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.12->llama-index-llms-gemini) (2.0.38)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.12->llama-index-llms-gemini) (3.11.12)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.12->llama-index-llms-gemini) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.12->llama-index-llms-gemini) (1.2.18)\n",
            "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.12->llama-index-llms-gemini) (1.0.8)\n",
            "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.12->llama-index-llms-gemini) (1.2.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.12->llama-index-llms-gemini) (2024.10.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.12->llama-index-llms-gemini) (0.28.1)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.12->llama-index-llms-gemini) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.12->llama-index-llms-gemini) (3.4.2)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.12->llama-index-llms-gemini) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.12->llama-index-llms-gemini) (1.26.4)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.12->llama-index-llms-gemini) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.12->llama-index-llms-gemini) (9.0.0)\n",
            "Requirement already satisfied: tiktoken>=0.3.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.12->llama-index-llms-gemini) (0.9.0)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.12->llama-index-llms-gemini) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.12->llama-index-llms-gemini) (1.17.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.12->llama-index-llms-gemini) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.12->llama-index-llms-gemini) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.12->llama-index-llms-gemini) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.12->llama-index-llms-gemini) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.12->llama-index-llms-gemini) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.12->llama-index-llms-gemini) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.12->llama-index-llms-gemini) (1.18.3)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai>=0.5.2->llama-index-llms-gemini) (1.67.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai>=0.5.2->llama-index-llms-gemini) (5.5.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai>=0.5.2->llama-index-llms-gemini) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai>=0.5.2->llama-index-llms-gemini) (4.9)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.12->llama-index-llms-gemini) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.12->llama-index-llms-gemini) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.12->llama-index-llms-gemini) (2024.11.6)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai>=0.5.2->llama-index-llms-gemini) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai>=0.5.2->llama-index-llms-gemini) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.12->llama-index-llms-gemini) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.12->llama-index-llms-gemini) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.12->llama-index-llms-gemini) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.12->llama-index-llms-gemini) (2025.1.31)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.12->llama-index-llms-gemini) (3.1.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.13.0,>=0.12.12->llama-index-llms-gemini) (1.0.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json->llama-index-core<0.13.0,>=0.12.12->llama-index-llms-gemini) (3.26.1)\n",
            "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai>=0.5.2->llama-index-llms-gemini) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai>=0.5.2->llama-index-llms-gemini) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai>=0.5.2->llama-index-llms-gemini) (4.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.12->llama-index-llms-gemini) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.12->llama-index-llms-gemini) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.13.0,>=0.12.12->llama-index-llms-gemini) (0.14.0)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai>=0.5.2->llama-index-llms-gemini) (1.70.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai>=0.5.2->llama-index-llms-gemini) (1.62.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai>=0.5.2->llama-index-llms-gemini) (3.2.1)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.11/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.13.0,>=0.12.12->llama-index-llms-gemini) (24.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai>=0.5.2->llama-index-llms-gemini) (0.6.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx->llama-index-core<0.13.0,>=0.12.12->llama-index-llms-gemini) (1.3.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install google-generativeai\n",
        "!pip install llama-index\n",
        "!pip install llama-index-embeddings-gemini\n",
        "!pip install llama-index-llms-gemini\n",
        "!pip install llama-index-vector-stores-chroma\n",
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "google_api_key=userdata.get(\"googleapikey\")"
      ],
      "metadata": {
        "id": "eaeZDJqIY9kn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "import os\n",
        "genai.configure(api_key=google_api_key)"
      ],
      "metadata": {
        "id": "gXMRzPvOZRt2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for model in genai.list_models():\n",
        "  print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "mm32TFyGZRjp",
        "outputId": "3d594ce8-bc49-4a48-8e95-319d78ce213f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model(name='models/chat-bison-001',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='PaLM 2 Chat (Legacy)',\n",
            "      description='A legacy text-only model optimized for chat conversations',\n",
            "      input_token_limit=4096,\n",
            "      output_token_limit=1024,\n",
            "      supported_generation_methods=['generateMessage', 'countMessageTokens'],\n",
            "      temperature=0.25,\n",
            "      max_temperature=None,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/text-bison-001',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='PaLM 2 (Legacy)',\n",
            "      description='A legacy model that understands text and generates text as an output',\n",
            "      input_token_limit=8196,\n",
            "      output_token_limit=1024,\n",
            "      supported_generation_methods=['generateText', 'countTextTokens', 'createTunedTextModel'],\n",
            "      temperature=0.7,\n",
            "      max_temperature=None,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/embedding-gecko-001',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Embedding Gecko',\n",
            "      description='Obtain a distributed representation of a text.',\n",
            "      input_token_limit=1024,\n",
            "      output_token_limit=1,\n",
            "      supported_generation_methods=['embedText', 'countTextTokens'],\n",
            "      temperature=None,\n",
            "      max_temperature=None,\n",
            "      top_p=None,\n",
            "      top_k=None)\n",
            "Model(name='models/gemini-1.0-pro-latest',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.0 Pro Latest',\n",
            "      description=('The original Gemini 1.0 Pro model. This model will be discontinued on '\n",
            "                   'February 15th, 2025. Move to a newer Gemini version.'),\n",
            "      input_token_limit=30720,\n",
            "      output_token_limit=2048,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=0.9,\n",
            "      max_temperature=None,\n",
            "      top_p=1.0,\n",
            "      top_k=None)\n",
            "Model(name='models/gemini-1.0-pro',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.0 Pro',\n",
            "      description='The best model for scaling across a wide range of tasks',\n",
            "      input_token_limit=30720,\n",
            "      output_token_limit=2048,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=0.9,\n",
            "      max_temperature=None,\n",
            "      top_p=1.0,\n",
            "      top_k=None)\n",
            "Model(name='models/gemini-pro',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.0 Pro',\n",
            "      description='The best model for scaling across a wide range of tasks',\n",
            "      input_token_limit=30720,\n",
            "      output_token_limit=2048,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=0.9,\n",
            "      max_temperature=None,\n",
            "      top_p=1.0,\n",
            "      top_k=None)\n",
            "Model(name='models/gemini-1.0-pro-001',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.0 Pro 001 (Tuning)',\n",
            "      description=('The original Gemini 1.0 Pro model version that supports tuning. Gemini 1.0 '\n",
            "                   'Pro will be discontinued on February 15th, 2025. Move to a newer Gemini '\n",
            "                   'version.'),\n",
            "      input_token_limit=30720,\n",
            "      output_token_limit=2048,\n",
            "      supported_generation_methods=['generateContent', 'countTokens', 'createTunedModel'],\n",
            "      temperature=0.9,\n",
            "      max_temperature=None,\n",
            "      top_p=1.0,\n",
            "      top_k=None)\n",
            "Model(name='models/gemini-1.0-pro-vision-latest',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.0 Pro Vision',\n",
            "      description=('The original Gemini 1.0 Pro Vision model version which was optimized for '\n",
            "                   'image understanding. Gemini 1.0 Pro Vision was deprecated on July 12, 2024. '\n",
            "                   'Move to a newer Gemini version.'),\n",
            "      input_token_limit=12288,\n",
            "      output_token_limit=4096,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=0.4,\n",
            "      max_temperature=None,\n",
            "      top_p=1.0,\n",
            "      top_k=32)\n",
            "Model(name='models/gemini-pro-vision',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.0 Pro Vision',\n",
            "      description=('The original Gemini 1.0 Pro Vision model version which was optimized for '\n",
            "                   'image understanding. Gemini 1.0 Pro Vision was deprecated on July 12, 2024. '\n",
            "                   'Move to a newer Gemini version.'),\n",
            "      input_token_limit=12288,\n",
            "      output_token_limit=4096,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=0.4,\n",
            "      max_temperature=None,\n",
            "      top_p=1.0,\n",
            "      top_k=32)\n",
            "Model(name='models/gemini-1.5-pro-latest',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.5 Pro Latest',\n",
            "      description=('Alias that points to the most recent production (non-experimental) release '\n",
            "                   'of Gemini 1.5 Pro, our mid-size multimodal model that supports up to 2 '\n",
            "                   'million tokens.'),\n",
            "      input_token_limit=2000000,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/gemini-1.5-pro-001',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.5 Pro 001',\n",
            "      description=('Stable version of Gemini 1.5 Pro, our mid-size multimodal model that '\n",
            "                   'supports up to 2 million tokens, released in May of 2024.'),\n",
            "      input_token_limit=2000000,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-1.5-pro-002',\n",
            "      base_model_id='',\n",
            "      version='002',\n",
            "      display_name='Gemini 1.5 Pro 002',\n",
            "      description=('Stable version of Gemini 1.5 Pro, our mid-size multimodal model that '\n",
            "                   'supports up to 2 million tokens, released in September of 2024.'),\n",
            "      input_token_limit=2000000,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/gemini-1.5-pro',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.5 Pro',\n",
            "      description=('Stable version of Gemini 1.5 Pro, our mid-size multimodal model that '\n",
            "                   'supports up to 2 million tokens, released in May of 2024.'),\n",
            "      input_token_limit=2000000,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/gemini-1.5-flash-latest',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.5 Flash Latest',\n",
            "      description=('Alias that points to the most recent production (non-experimental) release '\n",
            "                   'of Gemini 1.5 Flash, our fast and versatile multimodal model for scaling '\n",
            "                   'across diverse tasks.'),\n",
            "      input_token_limit=1000000,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/gemini-1.5-flash-001',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.5 Flash 001',\n",
            "      description=('Stable version of Gemini 1.5 Flash, our fast and versatile multimodal model '\n",
            "                   'for scaling across diverse tasks, released in May of 2024.'),\n",
            "      input_token_limit=1000000,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-1.5-flash-001-tuning',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.5 Flash 001 Tuning',\n",
            "      description=('Version of Gemini 1.5 Flash that supports tuning, our fast and versatile '\n",
            "                   'multimodal model for scaling across diverse tasks, released in May of 2024.'),\n",
            "      input_token_limit=16384,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens', 'createTunedModel'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-1.5-flash',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.5 Flash',\n",
            "      description=('Alias that points to the most recent stable version of Gemini 1.5 Flash, our '\n",
            "                   'fast and versatile multimodal model for scaling across diverse tasks.'),\n",
            "      input_token_limit=1000000,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/gemini-1.5-flash-002',\n",
            "      base_model_id='',\n",
            "      version='002',\n",
            "      display_name='Gemini 1.5 Flash 002',\n",
            "      description=('Stable version of Gemini 1.5 Flash, our fast and versatile multimodal model '\n",
            "                   'for scaling across diverse tasks, released in September of 2024.'),\n",
            "      input_token_limit=1000000,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/gemini-1.5-flash-8b',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.5 Flash-8B',\n",
            "      description=('Stable version of Gemini 1.5 Flash-8B, our smallest and most cost effective '\n",
            "                   'Flash model, released in October of 2024.'),\n",
            "      input_token_limit=1000000,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['createCachedContent', 'generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/gemini-1.5-flash-8b-001',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.5 Flash-8B 001',\n",
            "      description=('Stable version of Gemini 1.5 Flash-8B, our smallest and most cost effective '\n",
            "                   'Flash model, released in October of 2024.'),\n",
            "      input_token_limit=1000000,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['createCachedContent', 'generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/gemini-1.5-flash-8b-latest',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.5 Flash-8B Latest',\n",
            "      description=('Alias that points to the most recent production (non-experimental) release '\n",
            "                   'of Gemini 1.5 Flash-8B, our smallest and most cost effective Flash model, '\n",
            "                   'released in October of 2024.'),\n",
            "      input_token_limit=1000000,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['createCachedContent', 'generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/gemini-1.5-flash-8b-exp-0827',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.5 Flash 8B Experimental 0827',\n",
            "      description=('Experimental release (August 27th, 2024) of Gemini 1.5 Flash-8B, our '\n",
            "                   'smallest and most cost effective Flash model. Replaced by '\n",
            "                   'Gemini-1.5-flash-8b-001 (stable).'),\n",
            "      input_token_limit=1000000,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/gemini-1.5-flash-8b-exp-0924',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.5 Flash 8B Experimental 0924',\n",
            "      description=('Experimental release (September 24th, 2024) of Gemini 1.5 Flash-8B, our '\n",
            "                   'smallest and most cost effective Flash model. Replaced by '\n",
            "                   'Gemini-1.5-flash-8b-001 (stable).'),\n",
            "      input_token_limit=1000000,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/gemini-2.0-flash-exp',\n",
            "      base_model_id='',\n",
            "      version='2.0',\n",
            "      display_name='Gemini 2.0 Flash Experimental',\n",
            "      description='Gemini 2.0 Flash Experimental',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens', 'bidiGenerateContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/gemini-2.0-flash',\n",
            "      base_model_id='',\n",
            "      version='2.0',\n",
            "      display_name='Gemini 2.0 Flash',\n",
            "      description='Gemini 2.0 Flash',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/gemini-2.0-flash-001',\n",
            "      base_model_id='',\n",
            "      version='2.0',\n",
            "      display_name='Gemini 2.0 Flash 001',\n",
            "      description=('Stable version of Gemini 2.0 Flash, our fast and versatile multimodal model '\n",
            "                   'for scaling across diverse tasks, released in January of 2025.'),\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/gemini-2.0-flash-lite-preview',\n",
            "      base_model_id='',\n",
            "      version='preview-02-05',\n",
            "      display_name='Gemini 2.0 Flash-Lite Preview',\n",
            "      description='Preview release (February 5th, 2025) of Gemini 2.0 Flash Lite',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-2.0-flash-lite-preview-02-05',\n",
            "      base_model_id='',\n",
            "      version='preview-02-05',\n",
            "      display_name='Gemini 2.0 Flash-Lite Preview 02-05',\n",
            "      description='Preview release (February 5th, 2025) of Gemini 2.0 Flash Lite',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-2.0-pro-exp',\n",
            "      base_model_id='',\n",
            "      version='2.0',\n",
            "      display_name='Gemini 2.0 Pro Experimental',\n",
            "      description='Experimental release (February 5th, 2025) of Gemini 2.0 Pro',\n",
            "      input_token_limit=2097152,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-2.0-pro-exp-02-05',\n",
            "      base_model_id='',\n",
            "      version='2.0',\n",
            "      display_name='Gemini 2.0 Pro Experimental 02-05',\n",
            "      description='Experimental release (February 5th, 2025) of Gemini 2.0 Pro',\n",
            "      input_token_limit=2097152,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-exp-1206',\n",
            "      base_model_id='',\n",
            "      version='2.0',\n",
            "      display_name='Gemini Experimental 1206',\n",
            "      description='Experimental release (February 5th, 2025) of Gemini 2.0 Pro',\n",
            "      input_token_limit=2097152,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-2.0-flash-thinking-exp-01-21',\n",
            "      base_model_id='',\n",
            "      version='2.0-exp-01-21',\n",
            "      display_name='Gemini 2.0 Flash Thinking Experimental 01-21',\n",
            "      description='Experimental release (January 21st, 2025) of Gemini 2.0 Flash Thinking',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=65536,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=0.7,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-2.0-flash-thinking-exp',\n",
            "      base_model_id='',\n",
            "      version='2.0-exp-01-21',\n",
            "      display_name='Gemini 2.0 Flash Thinking Experimental 01-21',\n",
            "      description='Experimental release (January 21st, 2025) of Gemini 2.0 Flash Thinking',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=65536,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=0.7,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-2.0-flash-thinking-exp-1219',\n",
            "      base_model_id='',\n",
            "      version='2.0',\n",
            "      display_name='Gemini 2.0 Flash Thinking Experimental',\n",
            "      description='Gemini 2.0 Flash Thinking Experimental',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=65536,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=0.7,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/learnlm-1.5-pro-experimental',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='LearnLM 1.5 Pro Experimental',\n",
            "      description=('Alias that points to the most recent stable version of Gemini 1.5 Pro, our '\n",
            "                   'mid-size multimodal model that supports up to 2 million tokens.'),\n",
            "      input_token_limit=32767,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/embedding-001',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Embedding 001',\n",
            "      description='Obtain a distributed representation of a text.',\n",
            "      input_token_limit=2048,\n",
            "      output_token_limit=1,\n",
            "      supported_generation_methods=['embedContent'],\n",
            "      temperature=None,\n",
            "      max_temperature=None,\n",
            "      top_p=None,\n",
            "      top_k=None)\n",
            "Model(name='models/text-embedding-004',\n",
            "      base_model_id='',\n",
            "      version='004',\n",
            "      display_name='Text Embedding 004',\n",
            "      description='Obtain a distributed representation of a text.',\n",
            "      input_token_limit=2048,\n",
            "      output_token_limit=1,\n",
            "      supported_generation_methods=['embedContent'],\n",
            "      temperature=None,\n",
            "      max_temperature=None,\n",
            "      top_p=None,\n",
            "      top_k=None)\n",
            "Model(name='models/aqa',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Model that performs Attributed Question Answering.',\n",
            "      description=('Model trained to return answers to questions that are grounded in provided '\n",
            "                   'sources, along with estimating answerable probability.'),\n",
            "      input_token_limit=7168,\n",
            "      output_token_limit=1024,\n",
            "      supported_generation_methods=['generateAnswer'],\n",
            "      temperature=0.2,\n",
            "      max_temperature=None,\n",
            "      top_p=1.0,\n",
            "      top_k=40)\n",
            "Model(name='models/imagen-3.0-generate-002',\n",
            "      base_model_id='',\n",
            "      version='002',\n",
            "      display_name='Imagen 3.0 002 model',\n",
            "      description='Vertex served Imagen 3.0 002 model',\n",
            "      input_token_limit=480,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['predict'],\n",
            "      temperature=None,\n",
            "      max_temperature=None,\n",
            "      top_p=None,\n",
            "      top_k=None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for model in genai.list_models():\n",
        "  if \"embedContent\" in model.supported_generation_methods:\n",
        "    print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        },
        "id": "W7FDcHC9QHQu",
        "outputId": "a9119ed9-f3ee-4458-e310-726a72107d29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model(name='models/embedding-001',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Embedding 001',\n",
            "      description='Obtain a distributed representation of a text.',\n",
            "      input_token_limit=2048,\n",
            "      output_token_limit=1,\n",
            "      supported_generation_methods=['embedContent'],\n",
            "      temperature=None,\n",
            "      max_temperature=None,\n",
            "      top_p=None,\n",
            "      top_k=None)\n",
            "Model(name='models/text-embedding-004',\n",
            "      base_model_id='',\n",
            "      version='004',\n",
            "      display_name='Text Embedding 004',\n",
            "      description='Obtain a distributed representation of a text.',\n",
            "      input_token_limit=2048,\n",
            "      output_token_limit=1,\n",
            "      supported_generation_methods=['embedContent'],\n",
            "      temperature=None,\n",
            "      max_temperature=None,\n",
            "      top_p=None,\n",
            "      top_k=None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for model in genai.list_models():\n",
        "  if \"generateContent\" in model.supported_generation_methods:\n",
        "    print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3OPfHpYzZRgQ",
        "outputId": "4e68865f-0967-410f-e901-01c39d4f42dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model(name='models/gemini-1.0-pro-latest',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.0 Pro Latest',\n",
            "      description=('The original Gemini 1.0 Pro model. This model will be discontinued on '\n",
            "                   'February 15th, 2025. Move to a newer Gemini version.'),\n",
            "      input_token_limit=30720,\n",
            "      output_token_limit=2048,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=0.9,\n",
            "      max_temperature=None,\n",
            "      top_p=1.0,\n",
            "      top_k=None)\n",
            "Model(name='models/gemini-1.0-pro',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.0 Pro',\n",
            "      description='The best model for scaling across a wide range of tasks',\n",
            "      input_token_limit=30720,\n",
            "      output_token_limit=2048,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=0.9,\n",
            "      max_temperature=None,\n",
            "      top_p=1.0,\n",
            "      top_k=None)\n",
            "Model(name='models/gemini-pro',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.0 Pro',\n",
            "      description='The best model for scaling across a wide range of tasks',\n",
            "      input_token_limit=30720,\n",
            "      output_token_limit=2048,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=0.9,\n",
            "      max_temperature=None,\n",
            "      top_p=1.0,\n",
            "      top_k=None)\n",
            "Model(name='models/gemini-1.0-pro-001',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.0 Pro 001 (Tuning)',\n",
            "      description=('The original Gemini 1.0 Pro model version that supports tuning. Gemini 1.0 '\n",
            "                   'Pro will be discontinued on February 15th, 2025. Move to a newer Gemini '\n",
            "                   'version.'),\n",
            "      input_token_limit=30720,\n",
            "      output_token_limit=2048,\n",
            "      supported_generation_methods=['generateContent', 'countTokens', 'createTunedModel'],\n",
            "      temperature=0.9,\n",
            "      max_temperature=None,\n",
            "      top_p=1.0,\n",
            "      top_k=None)\n",
            "Model(name='models/gemini-1.0-pro-vision-latest',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.0 Pro Vision',\n",
            "      description=('The original Gemini 1.0 Pro Vision model version which was optimized for '\n",
            "                   'image understanding. Gemini 1.0 Pro Vision was deprecated on July 12, 2024. '\n",
            "                   'Move to a newer Gemini version.'),\n",
            "      input_token_limit=12288,\n",
            "      output_token_limit=4096,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=0.4,\n",
            "      max_temperature=None,\n",
            "      top_p=1.0,\n",
            "      top_k=32)\n",
            "Model(name='models/gemini-pro-vision',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.0 Pro Vision',\n",
            "      description=('The original Gemini 1.0 Pro Vision model version which was optimized for '\n",
            "                   'image understanding. Gemini 1.0 Pro Vision was deprecated on July 12, 2024. '\n",
            "                   'Move to a newer Gemini version.'),\n",
            "      input_token_limit=12288,\n",
            "      output_token_limit=4096,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=0.4,\n",
            "      max_temperature=None,\n",
            "      top_p=1.0,\n",
            "      top_k=32)\n",
            "Model(name='models/gemini-1.5-pro-latest',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.5 Pro Latest',\n",
            "      description=('Alias that points to the most recent production (non-experimental) release '\n",
            "                   'of Gemini 1.5 Pro, our mid-size multimodal model that supports up to 2 '\n",
            "                   'million tokens.'),\n",
            "      input_token_limit=2000000,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/gemini-1.5-pro-001',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.5 Pro 001',\n",
            "      description=('Stable version of Gemini 1.5 Pro, our mid-size multimodal model that '\n",
            "                   'supports up to 2 million tokens, released in May of 2024.'),\n",
            "      input_token_limit=2000000,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-1.5-pro-002',\n",
            "      base_model_id='',\n",
            "      version='002',\n",
            "      display_name='Gemini 1.5 Pro 002',\n",
            "      description=('Stable version of Gemini 1.5 Pro, our mid-size multimodal model that '\n",
            "                   'supports up to 2 million tokens, released in September of 2024.'),\n",
            "      input_token_limit=2000000,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/gemini-1.5-pro',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.5 Pro',\n",
            "      description=('Stable version of Gemini 1.5 Pro, our mid-size multimodal model that '\n",
            "                   'supports up to 2 million tokens, released in May of 2024.'),\n",
            "      input_token_limit=2000000,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/gemini-1.5-flash-latest',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.5 Flash Latest',\n",
            "      description=('Alias that points to the most recent production (non-experimental) release '\n",
            "                   'of Gemini 1.5 Flash, our fast and versatile multimodal model for scaling '\n",
            "                   'across diverse tasks.'),\n",
            "      input_token_limit=1000000,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/gemini-1.5-flash-001',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.5 Flash 001',\n",
            "      description=('Stable version of Gemini 1.5 Flash, our fast and versatile multimodal model '\n",
            "                   'for scaling across diverse tasks, released in May of 2024.'),\n",
            "      input_token_limit=1000000,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-1.5-flash-001-tuning',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.5 Flash 001 Tuning',\n",
            "      description=('Version of Gemini 1.5 Flash that supports tuning, our fast and versatile '\n",
            "                   'multimodal model for scaling across diverse tasks, released in May of 2024.'),\n",
            "      input_token_limit=16384,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens', 'createTunedModel'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-1.5-flash',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.5 Flash',\n",
            "      description=('Alias that points to the most recent stable version of Gemini 1.5 Flash, our '\n",
            "                   'fast and versatile multimodal model for scaling across diverse tasks.'),\n",
            "      input_token_limit=1000000,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/gemini-1.5-flash-002',\n",
            "      base_model_id='',\n",
            "      version='002',\n",
            "      display_name='Gemini 1.5 Flash 002',\n",
            "      description=('Stable version of Gemini 1.5 Flash, our fast and versatile multimodal model '\n",
            "                   'for scaling across diverse tasks, released in September of 2024.'),\n",
            "      input_token_limit=1000000,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/gemini-1.5-flash-8b',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.5 Flash-8B',\n",
            "      description=('Stable version of Gemini 1.5 Flash-8B, our smallest and most cost effective '\n",
            "                   'Flash model, released in October of 2024.'),\n",
            "      input_token_limit=1000000,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['createCachedContent', 'generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/gemini-1.5-flash-8b-001',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.5 Flash-8B 001',\n",
            "      description=('Stable version of Gemini 1.5 Flash-8B, our smallest and most cost effective '\n",
            "                   'Flash model, released in October of 2024.'),\n",
            "      input_token_limit=1000000,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['createCachedContent', 'generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/gemini-1.5-flash-8b-latest',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.5 Flash-8B Latest',\n",
            "      description=('Alias that points to the most recent production (non-experimental) release '\n",
            "                   'of Gemini 1.5 Flash-8B, our smallest and most cost effective Flash model, '\n",
            "                   'released in October of 2024.'),\n",
            "      input_token_limit=1000000,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['createCachedContent', 'generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/gemini-1.5-flash-8b-exp-0827',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.5 Flash 8B Experimental 0827',\n",
            "      description=('Experimental release (August 27th, 2024) of Gemini 1.5 Flash-8B, our '\n",
            "                   'smallest and most cost effective Flash model. Replaced by '\n",
            "                   'Gemini-1.5-flash-8b-001 (stable).'),\n",
            "      input_token_limit=1000000,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/gemini-1.5-flash-8b-exp-0924',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.5 Flash 8B Experimental 0924',\n",
            "      description=('Experimental release (September 24th, 2024) of Gemini 1.5 Flash-8B, our '\n",
            "                   'smallest and most cost effective Flash model. Replaced by '\n",
            "                   'Gemini-1.5-flash-8b-001 (stable).'),\n",
            "      input_token_limit=1000000,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/gemini-2.0-flash-exp',\n",
            "      base_model_id='',\n",
            "      version='2.0',\n",
            "      display_name='Gemini 2.0 Flash Experimental',\n",
            "      description='Gemini 2.0 Flash Experimental',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens', 'bidiGenerateContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/gemini-2.0-flash',\n",
            "      base_model_id='',\n",
            "      version='2.0',\n",
            "      display_name='Gemini 2.0 Flash',\n",
            "      description='Gemini 2.0 Flash',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/gemini-2.0-flash-001',\n",
            "      base_model_id='',\n",
            "      version='2.0',\n",
            "      display_name='Gemini 2.0 Flash 001',\n",
            "      description=('Stable version of Gemini 2.0 Flash, our fast and versatile multimodal model '\n",
            "                   'for scaling across diverse tasks, released in January of 2025.'),\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/gemini-2.0-flash-lite-preview',\n",
            "      base_model_id='',\n",
            "      version='preview-02-05',\n",
            "      display_name='Gemini 2.0 Flash-Lite Preview',\n",
            "      description='Preview release (February 5th, 2025) of Gemini 2.0 Flash Lite',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-2.0-flash-lite-preview-02-05',\n",
            "      base_model_id='',\n",
            "      version='preview-02-05',\n",
            "      display_name='Gemini 2.0 Flash-Lite Preview 02-05',\n",
            "      description='Preview release (February 5th, 2025) of Gemini 2.0 Flash Lite',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-2.0-pro-exp',\n",
            "      base_model_id='',\n",
            "      version='2.0',\n",
            "      display_name='Gemini 2.0 Pro Experimental',\n",
            "      description='Experimental release (February 5th, 2025) of Gemini 2.0 Pro',\n",
            "      input_token_limit=2097152,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-2.0-pro-exp-02-05',\n",
            "      base_model_id='',\n",
            "      version='2.0',\n",
            "      display_name='Gemini 2.0 Pro Experimental 02-05',\n",
            "      description='Experimental release (February 5th, 2025) of Gemini 2.0 Pro',\n",
            "      input_token_limit=2097152,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-exp-1206',\n",
            "      base_model_id='',\n",
            "      version='2.0',\n",
            "      display_name='Gemini Experimental 1206',\n",
            "      description='Experimental release (February 5th, 2025) of Gemini 2.0 Pro',\n",
            "      input_token_limit=2097152,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-2.0-flash-thinking-exp-01-21',\n",
            "      base_model_id='',\n",
            "      version='2.0-exp-01-21',\n",
            "      display_name='Gemini 2.0 Flash Thinking Experimental 01-21',\n",
            "      description='Experimental release (January 21st, 2025) of Gemini 2.0 Flash Thinking',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=65536,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=0.7,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-2.0-flash-thinking-exp',\n",
            "      base_model_id='',\n",
            "      version='2.0-exp-01-21',\n",
            "      display_name='Gemini 2.0 Flash Thinking Experimental 01-21',\n",
            "      description='Experimental release (January 21st, 2025) of Gemini 2.0 Flash Thinking',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=65536,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=0.7,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-2.0-flash-thinking-exp-1219',\n",
            "      base_model_id='',\n",
            "      version='2.0',\n",
            "      display_name='Gemini 2.0 Flash Thinking Experimental',\n",
            "      description='Gemini 2.0 Flash Thinking Experimental',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=65536,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=0.7,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/learnlm-1.5-pro-experimental',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='LearnLM 1.5 Pro Experimental',\n",
            "      description=('Alias that points to the most recent stable version of Gemini 1.5 Pro, our '\n",
            "                   'mid-size multimodal model that supports up to 2 million tokens.'),\n",
            "      input_token_limit=32767,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model=genai.GenerativeModel(\"models/gemini-1.5-flash\")\n",
        "response=model.generate_content(\"  What is the pathway for transitiong of a doctor to datascientist in healthcare\").text"
      ],
      "metadata": {
        "id": "NCbWv57QcJI4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap\n",
        "from IPython.display import Markdown\n",
        "def to_markdown(text):\n",
        "  text = text.replace('•', '  *')\n",
        "  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "input_text = \"This is a • sample text with bullet points.\"\n",
        "result = to_markdown(input_text)\n",
        "\n",
        "display(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 62
        },
        "id": "1fDdHZJndJC4",
        "outputId": "b250af0c-c4b8-405a-89d9-cd666480d22e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "> This is a   * sample text with bullet points."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "to_markdown(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 941
        },
        "id": "j76ByhoJdI4y",
        "outputId": "56499964-f585-4c8a-c305-695247525383"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "> The transition from doctor to healthcare data scientist isn't a single, well-defined path, but rather a journey that can be tailored to individual skills and experience.  Here's a potential pathway, broken down into key steps:\n> \n> **Phase 1: Self-Assessment and Skill Development:**\n> \n> 1. **Identify your niche:** What areas of healthcare data analysis interest you most (e.g., clinical trials, disease prediction, operational efficiency, public health)?  Focusing your efforts will make the transition more manageable.\n> \n> 2. **Assess your existing skills:**  You already possess valuable clinical knowledge.  Identify your strengths in areas like data interpretation, problem-solving, and critical thinking.  Determine where you need to build skills in:\n>     * **Programming:** Python (with libraries like Pandas, NumPy, Scikit-learn) and R are essential.\n>     * **Statistical analysis:** Hypothesis testing, regression analysis, statistical modeling.\n>     * **Machine learning:**  Supervised, unsupervised, and reinforcement learning techniques.\n>     * **Databases:** SQL and NoSQL databases are crucial for data manipulation and querying.\n>     * **Data visualization:** Tools like Tableau, Power BI, or Matplotlib/Seaborn are needed to communicate findings effectively.\n>     * **Cloud computing:** Familiarity with platforms like AWS, Azure, or GCP is beneficial for handling large datasets.\n> \n> 3. **Targeted skill acquisition:**\n>     * **Online courses:** Coursera, edX, Udacity, DataCamp, and others offer numerous data science courses. Look for those specifically tailored to healthcare.\n>     * **Bootcamps:** Intensive, shorter programs that offer hands-on training.  Choose one with a good reputation and focus on healthcare applications.\n>     * **Master's degree (optional):** A Master's in Biostatistics, Data Science, or Public Health with a data science focus can significantly enhance credentials.  However, a strong portfolio can often substitute for a formal degree.\n>     * **Self-study:** Utilize online resources, textbooks, and practice projects.\n> \n> **Phase 2: Building your Portfolio:**\n> \n> 1. **Personal projects:** Work on projects that demonstrate your skills.  This could involve analyzing publicly available healthcare datasets (e.g., from Kaggle or government sources), developing predictive models, or creating visualizations to communicate insights.\n> \n> 2. **Volunteer work:** Offer your data science skills to a non-profit organization or research group focused on healthcare.\n> \n> 3. **Networking:** Attend data science conferences and meetups, connect with professionals on LinkedIn, and engage in online communities.\n> \n> **Phase 3: Job Search and Transition:**\n> \n> 1. **Tailor your resume and cover letter:** Highlight your clinical experience and data science skills, emphasizing how your unique perspective can contribute to the role.\n> \n> 2. **Focus your job search:** Target roles in healthcare data science, such as data scientist, biostatistician, clinical data analyst, or health informaticist.\n> \n> 3. **Prepare for interviews:** Practice answering technical questions, showcasing your projects, and demonstrating your understanding of healthcare data.\n> \n> \n> **Important Considerations:**\n> \n> * **Ethical considerations:** Healthcare data is sensitive.  You must be familiar with HIPAA and other regulations related to data privacy and security.\n> * **Domain expertise:** Your medical background is a significant advantage.  Highlight this knowledge in your applications and interviews.\n> * **Networking is crucial:**  Leverage your existing professional network and build new connections within the data science community.\n> \n> \n> This transition takes time and dedication. Be patient, persistent, and focus on building a strong foundation in both clinical knowledge and data science skills.  The combination of medical expertise and data science prowess is highly valuable in the healthcare industry.\n"
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import SimpleDirectoryReader\n",
        "from llama_index.core import VectorStoreIndex\n",
        "\n",
        "documents=SimpleDirectoryReader(\"/content/new\").load_data()\n",
        "from llama_index.embeddings.gemini import GeminiEmbedding\n",
        "\n",
        "\n",
        "# get API key and create embeddings\n",
        "\n",
        "model_name = \"models/text-embedding-004\"\n",
        "\n",
        "\n",
        "embed_model = GeminiEmbedding(model_name=model_name, api_key=google_api_key)\n",
        "\n",
        "embeddings = embed_model.get_text_embedding(\"Google PaLM Embeddings.\")\n",
        "len(embeddings)"
      ],
      "metadata": {
        "id": "GhrE-DCNQ6Mf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "11c2d08c-5e65-4751-baf5-43de14a9d215"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "768"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Parse documents into smaller chunks"
      ],
      "metadata": {
        "id": "rup6-8E-oDke"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Global settings\n",
        "from llama_index.core import Settings\n",
        "from llama_index.llms.gemini import Gemini\n",
        "\n",
        "# # Local settings\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "\n",
        "index = VectorStoreIndex.from_documents(documents,embed_model=embed_model, transformations=[SentenceSplitter(chunk_size=800,chunk_overlap=200)])"
      ],
      "metadata": {
        "id": "Rmukh7v8nacR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index.storage_context.persist()"
      ],
      "metadata": {
        "id": "4eNmYAispQkT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming google_api_key is already defined\n",
        "llm = Gemini(api_key=google_api_key, model=\"models/gemini-1.5-flash\")\n",
        "Settings.llm = llm\n",
        "embed_model=GeminiEmbedding(model_name=\"models/text-embedding-004\",api_key=google_api_key)\n",
        "Settings.embed_model=embed_model"
      ],
      "metadata": {
        "id": "e0jGI3klIn8d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_engine = index.as_query_engine()\n",
        "response = query_engine.query(\"What is chickenpox iin pregnancy?\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "xIvAppmjIMPD",
        "outputId": "b6cd59cb-9900-4270-ece9-17db625bf082"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The provided text mentions \"Chickenpox in Pregnancy\" as the title of a guideline, but does not offer a definition of the condition itself.  Further information is available within the referenced guideline and studies.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chroma DB VectorStore"
      ],
      "metadata": {
        "id": "aQC9cGyLpTLz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import chromadb\n",
        "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
        "from llama_index.core import StorageContext\n",
        "\n",
        "chroma_client = chromadb.PersistentClient()\n",
        "chroma_collection = chroma_client.create_collection(\"quickstart\")\n",
        "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
        "storage_context = StorageContext.from_defaults(vector_store=vector_store)"
      ],
      "metadata": {
        "id": "7ps2tAf4pSZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_engine = index.as_query_engine()\n",
        "response = query_engine.query(\"What is chickenpox in pregnancy?\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "7qTHRYW9t0bi",
        "outputId": "d4736f4c-3a8f-436e-b591-44f4ba240592"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The provided text mentions chickenpox in pregnancy as the subject of a guideline.  Further details on the specifics of chickenpox during pregnancy are available within the referenced guideline and supporting research.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query_engine = index.as_query_engine()\n",
        "response = query_engine.query(\"What is the role of vaccine in chickenpox in pregnancy?\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "had1PvsNL-wn",
        "outputId": "65668e6f-3375-4f40-85ec-854d9da32255"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Postpartum vaccination may be considered if a pregnant woman is found to not have immunity to varicella-zoster virus (VZV).\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query_engine = index.as_query_engine()\n",
        "response = query_engine.query(\"What is obstetric cholestasis?\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "sFQGuxGGOH8u",
        "outputId": "cc28df73-1863-4c50-f99b-2c860a21fa82"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This question cannot be answered from the given source.\n",
            "\n"
          ]
        }
      ]
    }
  ]
}